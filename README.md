# Words Segmentation

This repository contains a pretokenizer that segments text into "words" for further processing.

We define three classes of tokens:

1. `C0` Control tokens (always atomic)
2. "Words" = runs of non-space, non-control + optional single trailing whitespace
3. Whitespace runs

For any script where the default is not suitable, you can implement a custom pretokenizer.
Modify `LANGUAGE_SPECS` in [languages.py](./words_segmentation/languages.py) to add a custom function for specific
scripts.

For example:

```python
LANGUAGE_SPECS: Dict[str, LanguageSpec] = {
    "Chinese": {
        "scripts": ("Han",),
        "callback": segment_chinese,
    },
    "Japanese": {
        "scripts": ("Han", "Hiragana", "Katakana"),
        "callback": segment_japanese,
    },
}
```

Then, with a `max_bytes` parameter, we split long words into smaller chunks while preserving
Unicode grapheme boundaries.

## Usage

Install:

```bash
pip install words-segmentation
```

Pretokenize text using a Huggingface Tokenizer implementation:

```python
from words_segmentation.tokenizer import WordsSegmentationTokenizer

pretokenizer = WordsSegmentationTokenizer(max_bytes=16)
tokens = pretokenizer.tokenize("hello world! æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦")
# ['hello ', 'world! ', 'æˆ‘', 'çˆ±', 'åŒ—äº¬', 'å¤©å®‰é—¨', ' ', 'ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦â€']
```

## [Writing systems without word boundaries](https://en.wikipedia.org/wiki/Category:Writing_systems_without_word_boundaries)

Perhaps there will come a day when we could have a universal pretokenizer that works for all languages.
Until then, we need to handle some writing systems with custom logic.
We implement custom fallback pretoknizers for the following writing systems:

- [x] [Chinese characters](https://en.wikipedia.org/wiki/Chinese_characters) -
  using [jieba](https://github.com/fxsjy/jieba)
- [x] [Japanese writing system](https://en.wikipedia.org/wiki/Japanese_writing_system) -
  using [fugashi](https://github.com/polm/fugashi)
- [ ] [Balinese script](https://en.wikipedia.org/wiki/Balinese_script)
- [ ] [Burmese alphabet](https://en.wikipedia.org/wiki/Burmese_alphabet)
- [ ] [Chá»¯ HÃ¡n](https://en.wikipedia.org/wiki/Ch%E1%BB%AF_H%C3%A1n)
- [ ] [Chá»¯ NÃ´m](https://en.wikipedia.org/wiki/Ch%E1%BB%AF_N%C3%B4m)
- [ ] [Hanja](https://en.wikipedia.org/wiki/Hanja)
- [ ] [Javanese script](https://en.wikipedia.org/wiki/Javanese_script)
- [ ] [Khmer script](https://en.wikipedia.org/wiki/Khmer_script)
- [ ] [Lao script](https://en.wikipedia.org/wiki/Lao_script)
- [ ] [Ê¼Phags-pa script](https://en.wikipedia.org/wiki/%CA%BCPhags-pa_script)
- [ ] [Rasm](https://en.wikipedia.org/wiki/Rasm)
- [ ] [Sawndip](https://en.wikipedia.org/wiki/Sawndip)
- [ ] [Scriptio continua](https://en.wikipedia.org/wiki/Scriptio_continua)
- [ ] [S'gaw Karen alphabet](https://en.wikipedia.org/wiki/S%27gaw_Karen_alphabet)
- [ ] [Tai Tham script](https://en.wikipedia.org/wiki/Tai_Tham_script)
- [x] [Thai script](https://en.wikipedia.org/wiki/Thai_script) -
  using [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)
- [ ] [Tibetan script](https://en.wikipedia.org/wiki/Tibetan_script)
- [ ] [Vietnamese alphabet](https://en.wikipedia.org/wiki/Vietnamese_alphabet)
- [ ] [Western Pwo alphabet](https://en.wikipedia.org/wiki/Western_Pwo_alphabet)

## Tokenization Parity

[Foroutan and Meister et al. (2025)](https://www.arxiv.org/pdf/2508.04796) note that:
> In multilingual models, the same meaning can take far more tokens in some languages,
> penalizing users of underrepresented languages with worse performance and higher API costs.

[![Tokenization Parity](assets/tokenization-parity.png)](https://www.linkedin.com/posts/sina-ahmadi-aba470287_dont-speak-english-you-must-pay-more-activity-7360959825893036035-vnFN)

Let's consider the same example, for whitespace pre-tokenization parity:

| Language | Text (Google Translate)                                                                                                                                                                                                                                      | Bytes (UTF-8) | Tokens (GPT-4) | Words (Whitespace+) |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|----------------|---------------------|
| English  | Tours are cheaper for larger groups, so if you're by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate.                                                                                | 173           | 40             | 34                  |
| Italian  | I tour sono piÃ¹ economici per i gruppi piÃ¹ numerosi, quindi se sei da solo o con un solo amico, prova a incontrare altre persone e a formare un gruppo da quattro a sei persone per ottenere una tariffa piÃ¹ conveniente a persona.                          | 230           | 58             | 43                  |
| German   | Touren sind fÃ¼r grÃ¶ÃŸere Gruppen gÃ¼nstiger. Wenn Sie also alleine oder mit nur einem Freund unterwegs sind, versuchen Sie, andere Leute kennenzulernen und eine Gruppe von vier bis sechs Personen zu bilden, um einen besseren Preis pro Person zu erhalten. | 256           | 64             | 40                  |
| Chinese  | å›¢ä½“æ—…æ¸¸ä»·æ ¼æ›´ä¾¿å®œï¼Œæ‰€ä»¥å¦‚æœæ‚¨ç‹¬è‡ªä¸€äººæˆ–åªæœ‰ä¸€ä¸ªæœ‹å‹ï¼Œè¯·å°è¯•ç»“è¯†å…¶ä»–äººå¹¶ç»„æˆä¸€ä¸ªå››åˆ°å…­äººçš„å›¢ä½“ï¼Œä»¥è·å¾—æ›´å¥½çš„æ¯äººä»·æ ¼ã€‚                                                                                                                                                                                                  | 177           | 64             | 34                  |
| Japanese | ãƒ„ã‚¢ãƒ¼ã¯ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¤šã‘ã‚Œã°å®‰ããªã‚‹ã®ã§ã€ä¸€äººã¾ãŸã¯å‹é”ã¨ã ã‘å‚åŠ ã™ã‚‹å ´åˆã¯ã€ä»–ã®äººã¨ä¼šã£ã¦4äººã‹ã‚‰6äººã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œã‚‹ã¨ã€ä¸€äººå½“ãŸã‚Šã®æ–™é‡‘ãŒå®‰ããªã‚Šã¾ã™ã€‚                                                                                                                                                                                | 227           | 74             | 48                  |
| Finnish  | Retket ovat halvempia suuremmille ryhmille, joten jos olet yksin tai vain yhden ystÃ¤vÃ¤n kanssa, yritÃ¤ tavata muita ihmisiÃ¤ ja muodosta neljÃ¤n tai kuuden hengen ryhmÃ¤ saadaksesi paremman hinnan per henkilÃ¶.                                                | 212           | 79             | 30                  |
| Russian  | Ğ¢ÑƒÑ€Ñ‹ Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ, ĞµÑĞ»Ğ¸ Ğ²Ñ‹ Ğ¾Ğ´Ğ½Ğ¸ Ğ¸Ğ»Ğ¸ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼, Ğ¿Ğ¾ÑÑ‚Ğ°Ñ€Ğ°Ğ¹Ñ‚ĞµÑÑŒ Ğ¿Ğ¾Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ğ¸Ñ‚ÑŒÑÑ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…-ÑˆĞµÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½ÑƒÑ Ñ†ĞµĞ½Ñƒ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.                              | 409           | 100            | 32                  |
| Arabic   | ØªÙƒÙˆÙ† Ø§Ù„Ø¬ÙˆÙ„Ø§Øª Ø£Ø±Ø®Øµ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŒ Ù„Ø°Ù„Ùƒ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ù…ÙØ±Ø¯Ùƒ Ø£Ùˆ Ù…Ø¹ ØµØ¯ÙŠÙ‚ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·ØŒ ÙØ­Ø§ÙˆÙ„ Ù…Ù‚Ø§Ø¨Ù„Ø© Ø£Ø´Ø®Ø§Øµ Ø¢Ø®Ø±ÙŠÙ† ÙˆØªØ´ÙƒÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒÙˆÙ†Ø© Ù…Ù† Ø£Ø±Ø¨Ø¹Ø© Ø¥Ù„Ù‰ Ø³ØªØ© Ø£Ø´Ø®Ø§Øµ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¹Ø± Ø£ÙØ¶Ù„ Ù„Ù„Ø´Ø®Øµ Ø§Ù„ÙˆØ§Ø­Ø¯.                                                                  | 341           | 140            | 33                  |
| Hebrew   | ×¡×™×•×¨×™× ×–×•×œ×™× ×™×•×ª×¨ ×œ×§×‘×•×¦×•×ª ×’×“×•×œ×•×ª ×™×•×ª×¨, ×›×š ×©×× ××ª× ×œ×‘×“ ××• ×¢× ×—×‘×¨ ××—×“ ×‘×œ×‘×“, × ×¡×• ×œ×¤×’×•×© ×× ×©×™× ××—×¨×™× ×•×œ×™×¦×•×¨ ×§×‘×•×¦×” ×©×œ ××¨×‘×¢×” ×¢×“ ×©×™×©×” ×× ×©×™× ×œ×§×‘×œ×ª ××—×™×¨ ×˜×•×‘ ×™×•×ª×¨ ×œ××“×.                                                                                                | 281           | 151            | 31                  |
| Greek    | ÎŸÎ¹ ÎµÎºÎ´ÏÎ¿Î¼Î­Ï‚ ÎµÎ¯Î½Î±Î¹ Ï†Î¸Î·Î½ÏŒÏ„ÎµÏÎµÏ‚ Î³Î¹Î± Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎµÏ‚ Î¿Î¼Î¬Î´ÎµÏ‚, Î¿Ï€ÏŒÏ„Îµ Î±Î½ ÎµÎ¯ÏƒÏ„Îµ Î¼ÏŒÎ½Î¿Î¹ ÏƒÎ±Ï‚ Î® Î¼Îµ Î­Î½Î±Î½ Î¼ÏŒÎ½Î¿ Ï†Î¯Î»Î¿, Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î®ÏƒÏ„Îµ Î½Î± Î³Î½Ï‰ÏÎ¯ÏƒÎµÏ„Îµ Î¬Î»Î»Î± Î¬Ï„Î¿Î¼Î± ÎºÎ±Î¹ Î½Î± ÏƒÏ‡Î·Î¼Î±Ï„Î¯ÏƒÎµÏ„Îµ Î¼Î¹Î± Î¿Î¼Î¬Î´Î± Ï„ÎµÏƒÏƒÎ¬ÏÏ‰Î½ Î­Ï‰Ï‚ Î­Î¾Î¹ Î±Ï„ÏŒÎ¼Ï‰Î½ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Ï„Î¹Î¼Î® Î±Î½Î¬ Î¬Ï„Î¿Î¼Î¿.                                     | 394           | 193            | 36                  |
| Tamil    | à®ªà¯†à®°à®¿à®¯ à®•à¯à®´à¯à®•à¯à®•à®³à¯à®•à¯à®•à¯ à®šà¯à®±à¯à®±à¯à®²à®¾à®•à¯à®•à®³à¯ à®®à®²à®¿à®µà®¾à®©à®µà¯ˆ, à®à®©à®µà¯‡ à®¨à¯€à®™à¯à®•à®³à¯ à®¤à®©à®¿à®¯à®¾à®•à®µà¯‹ à®…à®²à¯à®²à®¤à¯ à®’à®°à¯ à®¨à®£à¯à®ªà®°à¯à®Ÿà®©à¯‹ à®‡à®°à¯à®¨à¯à®¤à®¾à®²à¯, à®®à®±à¯à®±à®µà®°à¯à®•à®³à¯ˆà®šà¯ à®šà®¨à¯à®¤à®¿à®¤à¯à®¤à¯ à®¨à®¾à®©à¯à®•à¯ à®®à¯à®¤à®²à¯ à®†à®±à¯ à®ªà¯‡à®°à¯ à®•à¯Šà®£à¯à®Ÿ à®•à¯à®´à¯à®µà¯ˆ à®‰à®°à¯à®µà®¾à®•à¯à®•à®¿, à®’à®°à¯ à®¨à®ªà®°à¯à®•à¯à®•à¯ à®šà®¿à®±à®¨à¯à®¤ à®µà®¿à®²à¯ˆà®¯à¯ˆà®ªà¯ à®ªà¯†à®± à®®à¯à®¯à®±à¯à®šà®¿à®•à¯à®•à®µà¯à®®à¯.                                      | 587           | 293            | 26                  |
| Kannada  | à²¦à³Šà²¡à³à²¡ à²—à³à²‚à²ªà³à²—à²³à²¿à²—à³† à²ªà³à²°à²µà²¾à²¸à²—à²³à³ à²…à²—à³à²—à²µà²¾à²—à²¿à²°à³à²¤à³à²¤à²µà³†, à²†à²¦à³à²¦à²°à²¿à²‚à²¦ à²¨à³€à²µà³ à²’à²¬à³à²¬à²‚à²Ÿà²¿à²¯à²¾à²—à²¿ à²…à²¥à²µà²¾ à²’à²¬à³à²¬ à²¸à³à²¨à³‡à²¹à²¿à²¤à²¨à³Šà²‚à²¦à²¿à²—à³† à²‡à²¦à³à²¦à²°à³†, à²‡à²¤à²° à²œà²¨à²°à²¨à³à²¨à³ à²­à³‡à²Ÿà²¿ à²®à²¾à²¡à²²à³ à²ªà³à²°à²¯à²¤à³à²¨à²¿à²¸à²¿ à²®à²¤à³à²¤à³ à²ªà³à²°à²¤à²¿ à²µà³à²¯à²•à³à²¤à²¿à²—à³† à²‰à²¤à³à²¤à²® à²¦à²°à²•à³à²•à²¾à²—à²¿ à²¨à²¾à²²à³à²•à²°à²¿à²‚à²¦ à²†à²°à³ à²œà²¨à²° à²—à³à²‚à²ªà²¨à³à²¨à³ à²°à²šà²¿à²¸à²¿.                                              | 565           | 361            | 26                  |
| Shan     | á¶á¢á€á€ºá€¸á€á¢á€„á€ºá€¸ á€á‚ƒá‚‡á¸á€¯á€™á€ºá€¸á€šá‚‚á€ºá‚‡á¼á¼á€ºá‚‰ áµá‚ƒá‚ˆá¶á¼á€ºá€™á¼á€ºá€¸ á€‘á€¯áµá€ºá‚‡á€œá€­á€°á€á€ºá€œá‚„á‚ˆ á€á€„á€ºá€á‚ƒá‚ˆ á¸á€á€ºá‚ˆáµá€á€ºá‚‡ á€šá€°á‚‡á‚á€„á€ºá€¸áµá€°áºá€ºá€¸ á€¢á€™á€ºá‚‡á¼á¼á€º á€™á€®á€¸á€¢á€°áºá€ºá€¸áµá€±á‚ƒá‚‰ áµá€±á‚ƒá‚‰á€œá€µá€á€ºáµá€½á‚†á€¸á¼á‚†á¸á€­á€¯á€„á€º á¶á€á€ºá€¸á¸á‚‚á€º á‚á€°á€•á€ºá‚‰á€‘á€°á€•á€ºá€¸ áµá€°á¼á€ºá€¸á€á¢á€„á€ºá‚‡áµá€±á‚ƒá‚‰á€á€± á‚á€µá€á€ºá€¸á¸á€¯á€™á€ºá€¸ 4 áµá€±á‚ƒá‚‰ á€á€±á‚ƒá‚‡á€‘á€­á€¯á€„á€º 6 áµá€±á‚ƒá‚‰ á‚á‚‚á€ºá‚ˆá€œá‚†á‚ˆ áµá‚ƒá‚ˆá¶á¼á€º á¼á€­á€¯á€„á€ºá‚ˆáµá€±á‚ƒá‚‰ á€¢á¼á€ºá€œá€®á€œá€­á€°á€á€ºá¼á¼á€ºá‚‰á€šá€á€ºá‚‰á‹              | 669           | 531            | 23                  |

#### Bytes Efficiency

English really is the most efficient language in terms of bytes count, which is not suprising given its Latin alphabet,
without diacritics or ligatures (with 1 byte per character).
Other languages that use the Latin alphabet are also relatively efficient (e.g. Italian, German, Finnish), but their
use of diacritics and ligatures increases the byte count.

Languages that use non-Latin scripts (e.g. Arabic, Hebrew, Shan) have a much higher byte count, due to the need for
multiple bytes per character in UTF-8 encoding. Hebrew and Arabic use two bytes per character,
while Shan uses three bytes per character, not counting ligatures.

#### Tokenization Efficiency (GPT-4)

English is also the most efficient language in terms of token count, which is not suprising given that the tokenizer
was trained primarily on English text.
Other languages that use the Latin alphabet are also relatively efficient, but the moment we move to non-Latin scripts,
the token count increases significantly (up to 13x for Shan).

#### Words Efficiency

Assuming whitespace tokenization as a proxy for words, we see that English is not the most efficient language.
This makes sense, from a language efficiency perspective, that there is no computational bias towards English.
Languages distribute between 23 and 43 words for the same sentence, with English right in the middle with 34.

![Tokenization Parity - Words](assets/tokenization-parity-words.png)

## Cite

If you use this code in your research, please consider citing the work:

```bibtex
@misc{moryossef2025words,
  title={Words Segmentation: A Word Level Pre-tokenizer for Languages of the World},
  author={Moryossef, Amit},
  howpublished={\url{https://github.com/sign/words-segmentation}},
  year={2025}
}
```
